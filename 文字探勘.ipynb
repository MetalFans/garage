{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./workspace/data/listMerge_72838_13897_14568_v6.json') as f:\n",
    "    text = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import codecs\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'url', u'article', u'blog_type', u'_id']\n"
     ]
    }
   ],
   "source": [
    "print text[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in text:\n",
    "    if t.get('article'):\n",
    "        t['article'] = ' '.join(t['article'].split())\n",
    "    else:\n",
    "        t['article'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open('./Workspace/data/dict0610.txt', 'w', 'utf-8') as w:\n",
    "    with codecs.open('./Workspace/data/dict0608.txt', 'r', 'utf-8') as f:\n",
    "        for d in f.readlines():\n",
    "            d = re.sub(u'\\(.*\\)| [\\u2150-\\u218f]+|[,.-]','',d)\n",
    "            d = ' '.join(d.split())\n",
    "            if re.search(u'[\\u4e00-\\u9fff]+ [\\u4e00-\\u9fff]+ 7', d):\n",
    "                w.write(d.split()[0] + ' 7 n\\n')\n",
    "                w.write(d.split()[1] + ' 7 n\\n')\n",
    "            elif re.search(u\"[\\u4e00-\\u9fff]+ \\' 2\", d):\n",
    "                w.write(d.split()[0] + ' 2 n\\n')\n",
    "            else:\n",
    "                if not re.search('\\w', d.split()[0]):\n",
    "                    if len(d.strip().split())<3:\n",
    "                        w.write(d + ' n\\n')\n",
    "                    else:\n",
    "                        w.write(d + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/fan/anaconda/bin/Workspace/data/dict0610.txt ...\n",
      "DEBUG:jieba:Building prefix dict from /Users/fan/anaconda/bin/Workspace/data/dict0610.txt ...\n",
      "Loading model from cache /var/folders/ws/qq63c39d43l_20sybqcwcf900000gn/T/jieba.u242cdcfac3f46cc5b0cd775bf87c8643.cache\n",
      "DEBUG:jieba:Loading model from cache /var/folders/ws/qq63c39d43l_20sybqcwcf900000gn/T/jieba.u242cdcfac3f46cc5b0cd775bf87c8643.cache\n",
      "Loading model cost 0.693 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.693 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.set_dictionary('./Workspace/data/dict0610.txt')\n",
    "jieba.load_userdict(\"./Workspace/data/dictionary/all/dictionary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = []\n",
    "with open('./Workspace/data/sentence.txt', 'w') as f:\n",
    "    for t in text:\n",
    "        for s in t['article'].split():\n",
    "            cut = list(jieba.cut(s))\n",
    "            sentence.append(cut)\n",
    "            f.write(','.join(cut).encode('utf-8') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = gensim.models.Word2Vec(sentence, size=200, workers=2, min_count=1, sg=1)\n",
    "#model.save('./Workspace/data/mymodelSG')\n",
    "#model = gensim.models.Word2Vec(sentence, size=200, workers=2, min_count=1)\n",
    "#model.save('./Workspace/data/mymodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "model1 = gensim.models.Word2Vec.load('./Workspace/data/mymodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "model2 = gensim.models.Word2Vec.load('./Workspace/data/mymodelSG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "肯德基 0.795453131199\n",
      "摩斯漢堡 0.76553529501\n",
      "摩斯 0.65747743845\n",
      "COSTCO 0.65358543396\n",
      "便利商店 0.638280153275\n",
      "好市多 0.630457282066\n",
      "Costco 0.616374135017\n",
      "超商 0.597485899925\n",
      "MOS 0.594828605652\n",
      "星巴克 0.579089283943\n",
      "郵局 0.567501068115\n",
      "屈臣氏 0.55964422226\n",
      "大潤發 0.559318423271\n",
      "美而美 0.557879090309\n",
      "警察局 0.557409286499\n",
      "好樂迪 0.555902957916\n",
      "速食店 0.555692672729\n",
      "肯德雞 0.554739654064\n",
      "菜市場 0.55342912674\n",
      "圖書館 0.552685558796\n"
     ]
    }
   ],
   "source": [
    "for ele in model1.most_similar(u'麥當勞', topn=20):\n",
    "    print ele[0], ele[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "起司牛排 0.7633446455\n",
      "堡元 0.761631429195\n",
      "培根總匯 0.756853580475\n",
      "豪華總匯 0.754687786102\n",
      "起士豬排 0.754125952721\n",
      "肉蛋三明治 0.751661360264\n",
      "DanishPastryBakedwithMeatSauce 0.750438809395\n",
      "淺艇 0.749119579792\n",
      "THECHICAGO 0.748167812824\n",
      "警長 0.744097530842\n",
      "鱷梨 0.74230659008\n",
      "招牌總匯 0.742042958736\n",
      "重酥 0.74131411314\n",
      "牛肉起司堡 0.738449513912\n",
      "燻雞起士 0.7374958992\n",
      "培根起司漢堡 0.733188867569\n",
      "Cali 0.732995986938\n",
      "洛林鹹派 0.73259305954\n",
      "蔬菜三明治 0.73179513216\n",
      "威新 0.731755435467\n"
     ]
    }
   ],
   "source": [
    "for ele in model2.most_similar(u'吉士', topn=20):\n",
    "    print ele[0], ele[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5742fb2f692970b05a90abe3 http://leafphoto.pixnet.net/blog/post/114896723\n"
     ]
    }
   ],
   "source": [
    "for t in text:\n",
    "    if 'padding' in t['article']:\n",
    "        print t['_id'], t['url']\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import jieba\n",
    "# 負面\n",
    "negdict = []\n",
    "# 正面\n",
    "posdict = []\n",
    "# 否定\n",
    "nodict = []\n",
    "# 程度\n",
    "plusdict = []\n",
    "# 不肯定\n",
    "question = []\n",
    "with codecs.open('./Workspace/sentiment/negativewords.txt', 'r', 'utf-8') as f:\n",
    "    for w in f.readlines():\n",
    "        negdict.append(w.strip())\n",
    "with codecs.open('./Workspace/sentiment/positivewords.txt', 'r', 'utf-8') as f:\n",
    "    for w in f.readlines():\n",
    "        posdict.append(w.strip())\n",
    "with codecs.open('./Workspace/sentiment/negative.txt', 'r', 'utf-8') as f:\n",
    "    for w in f.readlines():\n",
    "        nodict.append(w.strip())\n",
    "with codecs.open('./Workspace/sentiment/more.txt', 'r', 'utf-8') as f:\n",
    "    for w in f.readlines():\n",
    "        plusdict.append(w.strip())\n",
    "with codecs.open('./Workspace/sentiment/question.txt', 'r', 'utf-8') as f:\n",
    "    for w in f.readlines():\n",
    "        question.append(w.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jb(s):\n",
    "    st = '|'.join(list(jieba.cut(s)))\n",
    "    return st\n",
    "def predict(s, negdict, posdict, nodict, plusdict):\n",
    "    p = 0\n",
    "    sd = list(jieba.cut(s))\n",
    "    pattern = u'文章|本文'\n",
    "    for i in range(len(sd)):\n",
    "        if re.search(pattern, ''.join(sd)) or sd[i] in question:\n",
    "            p = 0\n",
    "            break\n",
    "        elif sd[i] in negdict:\n",
    "            if i>0 and sd[i-1] in nodict:\n",
    "                p = p + 1\n",
    "            elif i>0 and sd[i-1] in plusdict:\n",
    "                p = p - 2\n",
    "            else: p = p - 1\n",
    "        elif sd[i] in posdict:\n",
    "            if i>0 and sd[i-1] in nodict:\n",
    "                p = p - 1\n",
    "            elif i>0 and sd[i-1] in plusdict:\n",
    "                p = p + 2\n",
    "            elif i>0 and sd[i-1] in negdict:\n",
    "                p = p - 1\n",
    "            elif i<len(sd)-1 and sd[i+1] in negdict:\n",
    "                p = p - 1\n",
    "            else: p = p + 1\n",
    "    return (sd, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35221\n",
      "和|風鮮 0\n",
      "屋 0\n",
      "台北市|大安|區|延吉街|號 0\n",
      "延吉街|實在|是|太多太多|日式|料理|了 0\n",
      "不過|我|跟|吃|飯|的|戰友|總有|機會|全部|吃|一輪 0\n",
      "這天來|到|和|風鮮 0\n",
      "屋 0\n",
      "小小的|店面 0\n",
      "一樣|沒有|什麼|裝潢 0\n",
      "是|很|容易|就|被|忽略|的|店家 0\n",
      "但|每次|經過|總是|高朋滿|座 0\n",
      "戰友|已經|先吃過|一次 0\n",
      "覺得|平價|又|划算 2\n",
      "就|陪|我|再|來|一次 0\n",
      "吧台|區|就可以看|見|老師傅|在|切著|新鮮|的|魚片 1\n",
      "看看|那|鮭魚 0\n",
      "那|油花|鐵定|是|好吃|的|阿 1\n",
      "MENU 0\n",
      "和|風鮮 0\n",
      "屋菜單|還算|多樣化 0\n",
      "雖然|店家|不|大 0\n",
      "但是|也|有|熟食|可選|喔 0\n",
      "這|天一|樣點|生魚|片蓋飯 0\n",
      "畢竟|上班族|族|是|要|吃|飽|一點|才|有|體力|生魚|片蓋飯|NT|蝦密 0\n",
      "和|風鮮 0\n",
      "屋|真的|是|在|忠孝|東|路上|轉個|彎|就|會|到|的|黃|金|地段|嗎 0\n",
      "這價格|在|這裡|出|現實|在|是|太|不可|思議|了 0\n",
      "難|怪人|會絡繹|不絕|得|來|壓 0\n",
      "太|划算|了 2\n",
      "生魚|片蓋飯|是|鮭魚 0\n",
      "鮪|魚 0\n",
      "旗魚|各|兩片 0\n",
      "不|知道|為|什麼 0\n",
      "照片|被|我|照得|很|難|吃|乾|乾|的|樣子 -2\n",
      "實際|上|沒|這麼|慘啦 0\n",
      "雖然|便宜|但魚|還是|有|一定|的|新|鮮度 1\n",
      "稱不上|山珍海味 0\n",
      "但|也|不|至於|太|失望 -2\n",
      "我個|人|還是|比|較|愛|鮭魚|就是|了 0\n",
      "底下|的|飯|不是|醋|飯 0\n",
      "聽聞|老客|戶|可以|跟|店家|要求 0\n",
      "不然|好像|不會|主動加|的|樣子|芥末|醬 0\n",
      "就|滿|一般|的|不多解|釋味|增魚湯|NT|魚湯|的|料|很多 0\n",
      "也|很|好喝 2\n",
      "不過|不建議|單點 0\n",
      "主餐 0\n",
      "就|可以|附湯|跟|沙拉|了 0\n",
      "馬|鈴薯|沙拉|NT 0\n",
      "元|可以|選|沙拉 0\n",
      "一樣|不建議|單點 0\n",
      "馬|鈴薯|泥磨|很細 0\n",
      "口感|綿密 1\n",
      "是|沒什麼|其他|特殊|的|配料 0\n",
      "就還|OK|生菜沙拉|NT|醬料|是|偏酸|的|那種 0\n",
      "吃|起來|很|清爽 2\n",
      "但|我|個|人|是|不|愛椒類|和|風鮮 0\n",
      "屋|就是|間|在|黃|金|地段|卻|主打|平價|日式|料理 1\n",
      "這天|吃|的|生魚|片蓋飯|才|NT 0\n",
      "就算|加|了|NT|變定|食|也|不過|NT 0\n",
      "我覺|得|直接|把|和|風鮮 0\n",
      "屋拿去|跟|高級|日式|料理|比較|根本|不|公平 0\n",
      "人家|本來|就算|是|家庭式|的|餐廳 0\n",
      "怎能|要求|精|緻|美味|與|平價|共存|呢 2\n",
      "我|不會|說|它|有|多|好吃 1\n",
      "但|這價位|實在|好|適合|偶爾想|吃生|魚片|的|小資|上班族|阿 0\n",
      "如果|想要|吃|好吃|的 1\n",
      "當然|就|點貴|一點|的|食材|囉 0\n",
      "我|相信|投資|報酬|率|還是|有|的 0\n",
      "對|了 0\n",
      "順帶|一提 0\n",
      "老|闆|人|很|好 0\n",
      "這樣|有加|分|嗎 0\n",
      "其他|日本料理 0\n",
      "延吉街|附近|其他|餐廳 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i = random.randint(0,len(text)-1)\n",
    "a = text[i]['article']\n",
    "sentences = a.split()\n",
    "print i\n",
    "for ele in sentences:\n",
    "    print jb(ele), predict(ele, negdict, posdict, nodict, plusdict)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(sd, negdict = negdict, posdict = posdict, nodict = nodict, plusdict = plusdict):\n",
    "    p = 0\n",
    "    pattern = u'文章|本文'\n",
    "    for i in range(len(sd)):\n",
    "        if re.search(pattern, ''.join(sd)) or sd[i] in question:\n",
    "            p = 0\n",
    "            break\n",
    "        elif sd[i] in negdict:\n",
    "            if i>0 and sd[i-1] in nodict:\n",
    "                p = p + 1\n",
    "            elif i>0 and sd[i-1] in plusdict:\n",
    "                p = p - 2\n",
    "            else: p = p - 1\n",
    "        elif sd[i] in posdict:\n",
    "            if i>0 and sd[i-1] in nodict:\n",
    "                p = p - 1\n",
    "            elif i>0 and sd[i-1] in plusdict:\n",
    "                p = p + 2\n",
    "            elif i>0 and sd[i-1] in negdict:\n",
    "                p = p - 1\n",
    "            elif i<len(sd)-1 and sd[i+1] in negdict:\n",
    "                p = p - 1\n",
    "            else: p = p + 1\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "container = []\n",
    "with codecs.open('./Workspace/data/sentence.txt', 'r', 'utf-8') as f:\n",
    "    for sentence in f.readlines():\n",
    "        container.append(sentence.strip().split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = set()\n",
    "neg = set()\n",
    "for ele in container:\n",
    "    result = predict(ele)\n",
    "    if result > 0:\n",
    "        [pos.add(s) for s in ele]\n",
    "    elif result < 0:\n",
    "        [neg.add(s) for s in ele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = list(pos)\n",
    "neg = list(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posv = [model2[ele] for ele in pos]\n",
    "negv = [model2[ele] for ele in neg]\n",
    "posd = dict((pos[i], i) for i in xrange(len(pos)))\n",
    "negd = dict((neg[i], i) for i in xrange(len(neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans_clustering = KMeans(n_clusters = 1000)\n",
    "nidx = kmeans_clustering.fit_predict(negv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nidx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5d0d41839db9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtempD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtempD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtempD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nidx' is not defined"
     ]
    }
   ],
   "source": [
    "r = dict(zip(pos, nidx))\n",
    "tempD = {}\n",
    "for ele in r:\n",
    "    if r[ele] not in tempD:\n",
    "        tempD[r[ele]] = [ele]\n",
    "    else:\n",
    "        tempD[r[ele]].append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "餐後出\n",
      "令人驚奇的\n",
      "披\n",
      "炸潭蝦\n",
      "嫩易\n",
      "物美\n",
      "多一筆\n",
      "紅豆沙牛奶\n",
      "些\n",
      "Brenda\n",
      "以解\n",
      "蘑菌\n",
      "二面\n",
      "猜超\n",
      "但京龍\n",
      "深挖\n",
      "摮\n",
      "咖哩雞法國麵包\n",
      "勞動\n",
      "這哦\n",
      "耍噱頭\n",
      "義大利水果麵包\n",
      "太強\n",
      "逗子\n",
      "小時候大餅\n",
      "原豆\n",
      "遮蔽\n",
      "廉\n",
      "把烤焦\n",
      "大典\n",
      "晶透感\n",
      "說少\n",
      "看嫩\n",
      "八還\n",
      "無骨\n",
      "冰人\n",
      "開中火\n",
      "嫩筍\n",
      "實型\n",
      "那面鐘\n",
      "張狂\n",
      "大多\n",
      "別跟我\n",
      "炸酥子\n",
      "元美姬\n",
      "大嵌\n",
      "倒不如\n",
      "掛著滿\n",
      "位能\n",
      "很綿滑\n",
      "白收\n",
      "PHATBurger\n",
      "不難想像\n",
      "普普\n",
      "報警\n",
      "大櫃\n",
      "幻想\n",
      "南山路\n",
      "路狗\n",
      "大浪\n",
      "我省\n",
      "蛇目菊\n",
      "周杰倫來\n",
      "身家\n",
      "希亞\n",
      "湯也夠\n",
      "這正\n",
      "這樣\n",
      "外樓\n",
      "YY\n",
      "鮮茄馬\n",
      "羊肉羹米粉\n",
      "價簾得\n",
      "白碗\n",
      "帶鮮\n",
      "領域\n",
      "不認\n",
      "Curry\n",
      "奶油雞\n",
      "知曉\n",
      "老城區\n",
      "買生\n",
      "先將\n",
      "綠椰汁雞肉\n",
      "一個人\n",
      "糖分\n",
      "中生代\n",
      "料區\n",
      "炸醬麵醬\n",
      "亂\n",
      "RYOUCAFE\n",
      "油好\n",
      "凹凸不平\n",
      "順間\n",
      "對帶\n",
      "芝新\n",
      "波段\n",
      "意念\n",
      "對加\n",
      "中型\n",
      "德大\n",
      "列式\n",
      "要花個\n",
      "中及\n",
      "VODKA\n",
      "衛生\n",
      "點愈貴\n",
      "人就點\n",
      "中滿\n",
      "ohhhh\n",
      "艷紅\n",
      "方呀\n",
      "六人份\n",
      "去掛\n",
      "德式香腸\n",
      "Bossanova\n",
      "地提\n",
      "細看\n",
      "番外篇\n",
      "甘藍\n",
      "雞毛\n",
      "鮮一\n",
      "皮加肉\n",
      "德氏\n",
      "腳踏實地\n",
      "封模\n",
      "白菜捲\n",
      "七千萬\n",
      "氣立牌\n",
      "淚奔\n",
      "印尼\n",
      "為會用\n",
      "妞\n",
      "火雞肉\n",
      "嫩燒\n",
      "令人喜悅的\n",
      "要和料\n",
      "伯爵鮮奶茶\n",
      "牛奶口味\n",
      "大牛哥\n",
      "先替\n",
      "湯也給\n",
      "圓鱈斷\n",
      "第二隻\n",
      "鰻嘴\n",
      "在台中\n",
      "滿足感\n",
      "中裡\n",
      "酥炸拼盤\n",
      "估\n",
      "惹人愛\n",
      "先送\n",
      "王義的\n",
      "玩樂\n",
      "娥\n",
      "所願\n",
      "兒科\n",
      "絕對不會\n",
      "去電\n",
      "董至\n",
      "再刷\n",
      "台南碗粿\n",
      "不負眾望\n",
      "忍者\n",
      "勾引\n",
      "Alisha\n",
      "細年\n",
      "地龍\n",
      "著作\n",
      "膩有\n",
      "醍魚\n",
      "味不濃\n",
      "手製巧克力\n",
      "鬼鍋\n",
      "要怕\n",
      "糖膏\n",
      "留耶\n",
      "加先\n",
      "大聲嚷嚷\n",
      "牛尾\n",
      "Krystal\n",
      "下鋪\n",
      "露手\n",
      "拆開來\n",
      "嬌鮮欲滴\n",
      "魚醋物\n",
      "南前\n",
      "廢物利用\n",
      "般軟\n",
      "赤崁\n",
      "帶書帶\n",
      "迷迭香麵包\n",
      "nogood\n",
      "石頭小火鍋\n",
      "魚漿\n",
      "立法院\n",
      "店禮\n",
      "十根\n",
      "蛋香\n",
      "臘味煲飯\n",
      "寶屋\n",
      "換飯\n",
      "褲以\n",
      "morton\n",
      "屋門\n",
      "聞一聞\n",
      "藥材店\n",
      "清甘挺\n",
      "往莊\n",
      "幾竟\n",
      "鬼\n",
      "以份\n",
      "量身打造\n",
      "和牛霜降\n",
      "高純度\n",
      "檀島\n",
      "看診\n",
      "大發慈悲\n",
      "壅塞\n",
      "指著\n",
      "瑩\n",
      "手工乾麵\n",
      "願不願意\n",
      "結交\n",
      "一兩口\n",
      "家庭手工\n",
      "外面\n",
      "不答\n",
      "追夢\n",
      "不空\n",
      "焦蔥豚骨拉麵\n",
      "帶略\n",
      "柏克\n",
      "蝦界裡\n",
      "不拘\n",
      "將新瀉\n",
      "Artifex\n",
      "膨潤\n",
      "往前走\n",
      "豪華海鮮丼\n",
      "餘則\n",
      "詩人\n",
      "爆烤\n",
      "跳元\n",
      "雜菜\n",
      "餐三\n",
      "HOKKI\n",
      "塔斯馬尼亞\n",
      "酸瓜\n",
      "邪門\n",
      "餐動線\n",
      "熱氣\n",
      "POLARCAF\n",
      "越換\n",
      "料非\n",
      "脆蔥\n",
      "梨園\n",
      "還上\n",
      "單吃餅\n",
      "摩艾頭\n",
      "海薰鹽\n",
      "來曜\n",
      "燉煮\n",
      "辣該\n",
      "黑木\n",
      "耐炸油\n",
      "PhoVenture\n",
      "歪果\n",
      "酥炸真\n",
      "點老\n",
      "黑給\n",
      "很常五\n",
      "不台味\n",
      "手打牛肉堡\n",
      "影城\n",
      "見過面\n",
      "少老\n",
      "紅脣\n",
      "贊多\n",
      "這兩首\n",
      "別甜\n",
      "化氮\n",
      "凌\n",
      "還頗贊\n",
      "山森原\n",
      "毐\n",
      "質加\n",
      "串醬燒\n",
      "光了裝\n"
     ]
    }
   ],
   "source": [
    "index = 22\n",
    "for ele in tempD[index]:\n",
    "    print ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-201-1c796775a68c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMatrixSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense2Corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/fan/anaconda/lib/python2.7/site-packages/gensim/similarities/docsim.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_best, dtype, num_features, chunksize, corpus_len)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scanning corpus to determine the number of features (consider setting `num_features` explicitly)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fan/anaconda/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36mget_max_id\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0mmaxid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mmaxid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfieldid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfieldid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [-1] to avoid exceptions from max(empty)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaxid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "smatrix = gensim.similarities.MatrixSimilarity(gensim.matutils.Dense2Corpus(model2.syn0.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99999976  0.57768422  0.51037735 ...,  0.51740605  0.48468122\n",
      "  0.47963852]\n",
      "[ 0.57768422  1.00000048  0.42085919 ...,  0.44845131  0.40958354\n",
      "  0.43441385]\n",
      "[ 0.51037735  0.42085919  0.99999994 ...,  0.41500497  0.37232333\n",
      "  0.39705241]\n",
      "[ 0.56345761  0.53546304  0.46664566 ...,  0.42936978  0.43303624\n",
      "  0.37481645]\n",
      "[ 0.51468313  0.4655771   0.48346111 ...,  0.44433281  0.44297996\n",
      "  0.45031035]\n"
     ]
    }
   ],
   "source": [
    "c = 1\n",
    "for sim in smatrix:\n",
    "    if c >5:\n",
    "        break\n",
    "    print sim\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    }
   ],
   "source": [
    "nsmatrix = gensim.similarities.MatrixSimilarity(gensim.matutils.Dense2Corpus(np.array(negv).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsarray = np.array(nsmatrix)\n",
    "nsarray[nsarray>1] = 1.0\n",
    "nsarray[nsarray<-1] = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "angularD = np.arccos(nsarray)/math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kernel = np.exp(- angularD ** 2 / (2. * 1. ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim = 1 - angularD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "spectral_clustering = SpectralClustering(n_clusters = 100)\n",
    "nsc = spectral_clustering.fit_predict(sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
